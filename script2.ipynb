{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available matplotlib backends: ['tk', 'gtk', 'gtk3', 'gtk4', 'wx', 'qt4', 'qt5', 'qt6', 'qt', 'osx', 'nbagg', 'notebook', 'agg', 'svg', 'pdf', 'ps', 'inline', 'ipympl', 'widget']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import IPython.display as disp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# list available backends\n",
    "# 'widget' backend allows interactive plots within the notebook\n",
    "# Matplotlib is convenient for zooming into the image and viewing pixel coordinates/values\n",
    "%matplotlib -l      \n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory : /Users/gautamd/Home/github/startrail-merger\n"
     ]
    }
   ],
   "source": [
    "SOURCE_PATH = glob.glob(\"./data/JPEG/*.jpg\")\n",
    "BASE_PATH   = \"./data/JPEG/4J7A6511.jpg\"\n",
    "\n",
    "print(\"Current Working Directory :\", os.getcwd())\n",
    "widgets.FileUpload(accept='image/*', multiple=True)\n",
    "\n",
    "SOURCE_PATH.sort()\n",
    "\n",
    "if not SOURCE_PATH:\n",
    "    raise FileNotFoundError(\"Please set a valid image sequence path for SOURCE_PATH\")\n",
    "if not os.path.isfile(BASE_PATH):\n",
    "    raise FileNotFoundError(\"Please set a valid image path for BASE_PATH\")\n",
    "\n",
    "\n",
    "_abs_source = list(map(os.path.abspath, SOURCE_PATH))\n",
    "if os.path.abspath(BASE_PATH) in _abs_source:\n",
    "    SOURCE_PATH.pop(_abs_source.index(os.path.abspath(BASE_PATH)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected dimensions of image batch (batchsize, w, h, bands) -  (None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "nn = tf.keras.models.load_model('data/CNNmodel')\n",
    "# print(dir(nn))\n",
    "print(\"Expected dimensions of image batch (batchsize, w, h, bands) - \", nn.input_shape)\n",
    "SLICE_SIZE = nn.input_shape[1:3]\n",
    "\n",
    "\n",
    "def preprocess(img, nw_offset=0, slice_size=SLICE_SIZE,):\n",
    "    \"\"\"Normalise the image colours; split it into multiple pieces\n",
    "    of required width/height & stack them into a 4D tensor.\n",
    "    Pad borders with the average colour in the image if dimensions\n",
    "    aren't an exact multiple.\"\"\"\n",
    "    h, w, bands = img.shape\n",
    "    dx, dy = slice_size\n",
    "    h += nw_offset; w += nw_offset\n",
    "    if bands > 3:\n",
    "        img = img[:,:,:3]\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    padding = tf.constant([\n",
    "      [nw_offset, dy-h%dy if h%dy else 0], \n",
    "      [nw_offset, dx-w%dx if w%dx else 0], \n",
    "      [0, 0]\n",
    "    ])\n",
    "    full = tf.pad(img, padding,\n",
    "      constant_values=tf.math.reduce_mean(img))\n",
    "    # plt.figure(); plt.imshow(tf.keras.utils.array_to_img(full))\n",
    "    slices = []\n",
    "    for y in range(0,h,dy):\n",
    "        for x in range(0,w,dx):\n",
    "            slices.append(full[y:y+dy, x:x+dx, :])\n",
    "\n",
    "    return tf.stack(slices)\n",
    "\n",
    "\n",
    "def reconstruct(img_stack, w, h, nw_offset=0, norm_factor=1):\n",
    "    \"\"\"Unstack the 4D output of the neural network into a 3D tensor\n",
    "    (output image), the reverse of `reconstruct`.\n",
    "    Also normalise intensity of colours in the output.\"\"\"\n",
    "    ny = np.ceil((h+nw_offset)/SLICE_SIZE[0]).astype(int)\n",
    "    nx = np.ceil((w+nw_offset)/SLICE_SIZE[1]).astype(int)\n",
    "    reconstructed = tf.concat([\n",
    "        tf.concat([\n",
    "            img_stack[x] for x in range(nx*y, nx*(y+1))\n",
    "        ], axis=1)\n",
    "        for y in range(ny)\n",
    "    ], axis=0)\n",
    "    og_shape = reconstructed[nw_offset : h+nw_offset, \n",
    "                             nw_offset : w+nw_offset, :]\n",
    "    mi, ma = og_shape.numpy().min(), og_shape.numpy().max()\n",
    "    if ma==mi :\n",
    "        processed = og_shape * 0\n",
    "    else :\n",
    "        processed = tf.clip_by_value((og_shape-mi) * norm_factor/(ma-mi),\n",
    "                          0.0, 1.0)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da596f4ef2fa470cb2cbb127c6722b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082301c7efb747ad86c7f6f7bed30e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fde406ea890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgStack = tf.io.decode_image(tf.io.read_file(BASE_PATH))\n",
    "\n",
    "for p in tqdm(SOURCE_PATH) :\n",
    "    img = tf.io.decode_image(tf.io.read_file(p))\n",
    "    h, w, bands = img.shape\n",
    "    mask_a = reconstruct(nn(preprocess(img, 0)), w, h, 0, 2)\n",
    "    mask_b = reconstruct(nn(preprocess(img, 64)), w, h, 64, 2)\n",
    "    mask = tf.math.maximum(mask_a, mask_b)\n",
    "\n",
    "    # plt.figure(); plt.imshow(tf.keras.utils.array_to_img(mask))\n",
    "\n",
    "    select = tf.cast(tf.cast(img, mask.dtype) * mask, img.dtype)\n",
    "    imgStack = tf.math.maximum(imgStack, select)\n",
    "\n",
    "plt.figure(); plt.imshow(tf.keras.utils.array_to_img(imgStack))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
